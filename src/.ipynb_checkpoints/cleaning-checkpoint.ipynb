{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91bd400c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in /Users/usuari/miniconda3/envs/ironhack/lib/python3.11/site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/usuari/miniconda3/envs/ironhack/lib/python3.11/site-packages (from bs4) (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/usuari/miniconda3/envs/ironhack/lib/python3.11/site-packages (from beautifulsoup4->bs4) (2.5)\n",
      "Requirement already satisfied: gender-guesser in /Users/usuari/miniconda3/envs/ironhack/lib/python3.11/site-packages (0.4.0)\n"
     ]
    }
   ],
   "source": [
    "# Importing libraries.\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "!pip install bs4\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import spacy\n",
    "!pip install gender-guesser\n",
    "import gender_guesser.detector as gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63c9c0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def author_name(string):\n",
    "        standardized_name = re.sub(r'\\s+', '_', string)\n",
    "        # Add a period (.) before each uppercase letter (except the first one and spaces)\n",
    "        standardized_name = re.sub(r'(?<=[a-zA-Z])(?=[A-Z])', '._', standardized_name)\n",
    "        # Add a period (.) before each uppercase letter following a period\n",
    "        standardized_name = re.sub(r'\\.(?=[A-Z])', '._', standardized_name)\n",
    "        \n",
    "        if standardized_name == 'Newt_Scamander/J._K._Rowling':\n",
    "            return 'J._K._Rowling'\n",
    "        else:\n",
    "            return standardized_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab274fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_genres(string):\n",
    "    if 'Horror' in string:\n",
    "        return 'Horror'\n",
    "    elif 'Science Fiction' and 'Fantasy' in string:\n",
    "        return 'Fantasy & Science Fiction'\n",
    "    elif 'Fiction' and 'Historical' in string:\n",
    "        return 'Historical novel'\n",
    "    elif 'Fiction' and 'Thriller' in string:\n",
    "        return 'Thriller'\n",
    "    elif '20th Century' in string:\n",
    "        return '20th Century Fiction'\n",
    "    elif 'Classics' in string:\n",
    "        return 'Classics'\n",
    "    elif 'Science Fiction' in string:\n",
    "        return 'Science Fiction'\n",
    "    elif 'Contemporary' in string:\n",
    "        return 'Contemporary Fiction'\n",
    "    elif 'Erotica' in string:\n",
    "        return 'Erotica'\n",
    "    elif 'European' and 'Fiction':\n",
    "        return 'European Literature'\n",
    "    else:\n",
    "        return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49ba68cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def birthplace_f(string):\n",
    "    name = string\n",
    "    base_url = 'https://en.wikipedia.org/wiki'\n",
    "    endpoint = '/' + name\n",
    "    url = base_url + endpoint\n",
    "    \n",
    "    try:\n",
    "        res = requests.get(url)\n",
    "        res.raise_for_status()  # Check for request success\n",
    "        time.sleep(2)\n",
    "        soup = BeautifulSoup(res.content, 'html.parser')\n",
    "        \n",
    "        # Attempt to find the birthplace information\n",
    "        birth_raw = soup.find(\"td\", {\"class\": \"infobox-data\"})\n",
    "        \n",
    "        if birth_raw is not None:\n",
    "            birth_info = birth_raw.getText().replace(\"(\", \"\\n\").replace(\")\", \"\\n\")\n",
    "            birth_info = re.sub(r'(\\d+)', r'\\1\\n', birth_info)\n",
    "            birth_list = birth_info.split('\\n')\n",
    "            birth_list[-1]\n",
    "            author_birthplace = birth_list[-1]\n",
    "            return author_birthplace\n",
    "        else:\n",
    "            return \"Birthplace information not found\"\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return \"Error: Unable to fetch Wikipedia page\"\n",
    "    except Exception as e:\n",
    "        return \"An error occurred: \" + str(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f94290c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def birthyear_f(string):\n",
    "    name = string\n",
    "    base_url = 'https://en.wikipedia.org/wiki'\n",
    "    endpoint = '/' + name\n",
    "    url = base_url + endpoint\n",
    "    \n",
    "    try:\n",
    "        res = requests.get(url)\n",
    "        res.raise_for_status()  # Check for request success\n",
    "        time.sleep(4)\n",
    "        soup = BeautifulSoup(res.content, 'html.parser')\n",
    "        \n",
    "        # Attempt to find the birthplace information\n",
    "        birth_raw = soup.find(\"td\", {\"class\": \"infobox-data\"})\n",
    "        \n",
    "        if birth_raw is not None:\n",
    "            birth_info = birth_raw.getText().replace(\"(\", \"\\n\").replace(\")\", \"\\n\")\n",
    "            birth_info = re.sub(r'(\\d+)', r'\\1\\n', birth_info)\n",
    "            birth_list = birth_info.split('\\n')\n",
    "            birth_year = birth_list[1]\n",
    "            return birth_year\n",
    "        else:\n",
    "            return \"Birthyear information not found\"\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return \"Error: Unable to fetch Wikipedia page\"\n",
    "    except Exception as e:\n",
    "        return \"An error occurred: \" + str(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24242ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the detector\n",
    "d = gender.Detector()\n",
    "\n",
    "def guess_gender(string):\n",
    "    names = string.split('_')\n",
    "    if names:\n",
    "        first_name = names[0]\n",
    "        # Guess the gender\n",
    "        gender = d.get_gender(first_name)\n",
    "        return gender\n",
    "    else:\n",
    "        return 'unknown'  # Handle cases where the input is not a valid full name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db828287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the detector\n",
    "d = gender.Detector()\n",
    "\n",
    "def guess_gender_new(string):\n",
    "    names = string.split('_')\n",
    "    if string == 'Wilkie_Collins':\n",
    "        return 'male'\n",
    "    elif string == 'Raynold_Gideon':\n",
    "        return 'male'\n",
    "    elif string == 'Laurell_K._Hamilton':\n",
    "        return 'female'\n",
    "    elif string == 'C._S._Friedman':\n",
    "        return 'female'\n",
    "    elif string == 'L._A._Banks':\n",
    "        return 'female'\n",
    "    elif string == 'Tananarive_Due':\n",
    "        return 'female'\n",
    "    elif string == 'V._C._Andrews':\n",
    "        return 'female'\n",
    "    elif string == 'Sandy_Petersen':\n",
    "        return 'male'\n",
    "    elif string == 'Mary_Higgins_Clark':\n",
    "        return 'female'  \n",
    "    elif string == 'Charlie_Huston':\n",
    "        return 'male'\n",
    "    elif string == 'Natsuo_Kirino':\n",
    "        return 'female'\n",
    "    else:\n",
    "        first_name = names[0]\n",
    "        # Guess the gender\n",
    "        gender = d.get_gender(first_name)\n",
    "        return gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8fdaa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_before_webscraping():\n",
    "    \n",
    "    # I explain here the steps previous to the web scraping process.\n",
    "    \n",
    "    # Import csv file.\n",
    "    books = pd.read_csv(\"/Users/usuari/Desktop/Ironhack/BOOTCAMP/projects/project-II/Data/books_genres.csv\",on_bad_lines='skip')\n",
    "    \n",
    "    # Rename columns\n",
    "    books = books.rename(columns={'  num_pages': 'num_pages'})\n",
    "    books.columns = [i.lower().replace(\" \", \"_\") for i in books.columns]\n",
    "    \n",
    "    # Dropping two useless columns\n",
    "    books.drop(['isbn13', 'isbn'], axis=1, inplace=True)\n",
    "    \n",
    "    # Creating a new column\n",
    "    pattern = r'(\\d{4})'\n",
    "    books['years'] = books['publication_date'].str.extract(pattern)\n",
    "    \n",
    "    # Applying the function written above\n",
    "    books['author'] = books['author'].apply(author_name)\n",
    "    \n",
    "    # Creating a new dataframe\n",
    "    book_author = books[['title', 'author']]\n",
    "    \n",
    "    # From this new dataframe I'm going to create another one, in which the rows with multiple authors only keep the first author.\n",
    "\n",
    "    new_rows = []\n",
    "\n",
    "    for index, row in book_author.iterrows():\n",
    "        authors = row['author'].split('/')\n",
    "        if len(authors) > 1:\n",
    "        # If there are multiple authors, I keep only the first author\n",
    "            first_author = authors[0]\n",
    "            new_row = {'title': row['title'], 'author': first_author}\n",
    "            new_rows.append(new_row)\n",
    "        else:\n",
    "        # If there is only one author, keep the original row\n",
    "            new_rows.append({'title': row['title'], 'author': row['author']})\n",
    "\n",
    "    # The new DataFrame with the updated rows\n",
    "    new_books = pd.DataFrame(new_rows)\n",
    "\n",
    "    # Reset the index\n",
    "    new_books.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    #I add the columns from the original dataframe that interest me.\n",
    "\n",
    "    new_books['average_rating'] = books['average_rating']\n",
    "    new_books['language_code'] = books['language_code']\n",
    "    new_books['num-pages'] = books['num_pages']\n",
    "    new_books['ratings_count'] = books['ratings_count']\n",
    "    new_books['text_reviews_count'] = books['text_reviews_count']\n",
    "    new_books['publisher'] = books['publisher']\n",
    "    new_books['genres'] = books['genres']\n",
    "    new_books['years'] = books['years']\n",
    "    \n",
    "    # I create three empty columns in this new dataframe (which I will fill afterwards with web scraping data from wikipedia): \n",
    "    # 'author_birthplace', 'author_birthdate', 'author_gender'.\n",
    "\n",
    "    new_books['author_birthplace'] = None\n",
    "    new_books['author_birthdate'] = None\n",
    "    new_books['author_gender'] = None\n",
    "    \n",
    "    # I'm going to drop the names of the authors that are not in a latin alphabet.\n",
    "\n",
    "    latin_pattern = re.compile(r'^[a-zA-Z_. ]+$')  \n",
    "    lat_books = new_books[new_books['author'].str.match(latin_pattern)]\n",
    "    \n",
    "    # I'm going to drop the rows in which the column of 'ratings_count' are less than 1000, because I want to analyze the most rated books.\n",
    "\n",
    "    lat_books = lat_books[lat_books['ratings_count'] >= 1000]\n",
    "\n",
    "    # I want to focuse on novels, so I'm going to drop the short stories:\n",
    "\n",
    "    lat_books = lat_books.drop(lat_books[lat_books['genres'].str.contains('Short Stories')].index)\n",
    "\n",
    "    # I also want to focuse on Fiction:\n",
    "\n",
    "    lat_books = lat_books.drop(lat_books[lat_books['genres'].str.contains('Nonfiction')].index)\n",
    "\n",
    "    # I want to exclude comics and manga:\n",
    "\n",
    "    lat_books = lat_books.drop(lat_books[lat_books['genres'].str.contains('Comics')].index)\n",
    "    lat_books = lat_books.drop(lat_books[lat_books['genres'].str.contains('Manga')].index)\n",
    "\n",
    "    #I'm goint to exclude Poetry as well:\n",
    "\n",
    "    lat_books = lat_books.drop(lat_books[lat_books['genres'].str.contains('Poetry')].index)\n",
    "\n",
    "    # and I'll exclude plays:\n",
    "\n",
    "    lat_books = lat_books.drop(lat_books[lat_books['genres'].str.contains('Plays')].index)\n",
    "\n",
    "    #I'm also Childrens' Animal and Picture Books:\n",
    "\n",
    "    lat_books = lat_books.drop(lat_books[lat_books['genres'].str.contains('Animals')].index)\n",
    "    lat_books = lat_books.drop(lat_books[lat_books['genres'].str.contains('Picture Books')].index)\n",
    "\n",
    "    # I don't want any set but individual books:\n",
    "    lat_books = lat_books.drop(lat_books[lat_books['title'].str.contains('Set')].index)\n",
    "    lat_books = lat_books.drop(lat_books[lat_books['title'].str.contains('Collection')].index)\n",
    "    \n",
    "    # I apply the function above to clean book genres:\n",
    "    lat_books['genres'] = lat_books['genres'].apply(clean_genres)\n",
    "   \n",
    "    return lat_books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48f10ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transforming_webscraping(lat_books):\n",
    "    \n",
    "    # Since it's almost Halloween, I'm going to create a new dataframe with only Horror novels.\n",
    "    horror_books = lat_books[lat_books['genres'] == 'Horror']\n",
    "    \n",
    "    # I'm going to keep only the horror novels that are better rated, above 3.9\n",
    "    horror_books = horror_books[horror_books['average_rating'] >= 3.9]\n",
    "    \n",
    "    # Webscraping from Wikipedia with the above function to fill the 'author_birthdate' column.\n",
    "    horror_books['author_birthdate'] = horror_books['author'].apply(birthyear_f)\n",
    "    \n",
    "    # Webscraping from Wikipedia with another function above to fill the 'author_birthplace' column.\n",
    "    horror_books['author_birthplace'] = horror_books['author'].apply(birthplace_f)\n",
    "    \n",
    "    # I apply a function above to fill the 'author_gender' column.\n",
    "    horror_books['author_gender'] = horror_books['author'].apply(guess_gender)\n",
    "    \n",
    "    # With another function, I correct the unknown values from the 'author_gender' column.\n",
    "    unknown_rows['author_gender'] = unknown_rows['author'].apply(gender_new)\n",
    "    \n",
    "    \n",
    "    # I save my dataframe with the information from web scraping saved, just in case.\n",
    "\n",
    "    horror_books.to_csv(\"horror_books_2.csv\", index=False)\n",
    "\n",
    "    # Specify the folder path and filename for the CSV file\n",
    "    folder_path = \"/Users/usuari/Desktop/Ironhack/BOOTCAMP/projects/project-II/Data\"\n",
    "    file_name = \"horror_books_2.csv\"\n",
    "\n",
    "    # Combine the folder path and filename to create the full file path\n",
    "    full_file_path = f\"{folder_path}/{file_name}\"\n",
    "\n",
    "    # Export the DataFrame to the specified folder\n",
    "    horror_books.to_csv(full_file_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ironhack",
   "language": "python",
   "name": "ironhack"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
